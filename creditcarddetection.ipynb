# -*- coding: utf-8 -*-
"""CreditCarddetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nzdh0CX-guxkB84cKZ_IQYcY4shHpY5R
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset (adjust the path if necessary)
data = pd.read_csv('fraudTrain.csv')  # If uploaded via Colab, you should use the file name directly

# Check the first few rows
print(data.head())

# Check for missing values
print(data.isnull().sum())

# Convert categorical variables into dummy/indicator variables if necessary
# For example, assuming 'category' and 'gender' are categorical
data = pd.get_dummies(data, columns=['category', 'gender'], drop_first=True)

# Convert 'long' column to numeric, handling errors
# Check if the column is already numeric
if not pd.api.types.is_numeric_dtype(data['long']):
    data['long'] = pd.to_numeric(data['long'].str.replace('-', ''), errors='coerce')

# Fill missing values in 'long' with the mean
data['long'].fillna(data['long'].mean(), inplace=True)

# Assuming 'is_fraud' is your target variable, create the feature matrix X and target variable y
X = data.drop('is_fraud', axis=1)  # Replace 'is_fraud' with your actual target variable name
y = data['is_fraud']

# Standardize the numerical features (such as 'amt', 'lat', 'long')
scaler = StandardScaler()
X[['amt', 'lat', 'long']] = scaler.fit_transform(X[['amt', 'lat', 'long']])

# Assuming 'datetime_column' is the name of your datetime column
X = data.drop(['is_fraud', 'trans_date_trans_time'], axis=1)

data['year'] = pd.to_datetime(data['trans_date_trans_time'], format='%d-%m-%Y %H:%M').dt.year
# Added format='%d-%m-%Y %H:%M' to specify the correct format of the date and time strings

# Define the reference date
reference_date = pd.to_datetime('2020-01-01')  # Choose an appropriate reference date

# Calculate time since reference date
# Changed the format to '%d-%m-%Y %H:%M' to match the actual date format
data['time_since_reference'] = (pd.to_datetime(data['trans_date_trans_time'], format='%d-%m-%Y %H:%M') - reference_date).dt.days

# Check for infinite values in numeric columns only
for col in X_train.select_dtypes(include=np.number).columns:
    if np.isinf(X_train[col]).any():
        X_train[col][np.isinf(X_train[col])] = np.nan
        X_train[col].fillna(X_train[col].mean(), inplace=True)

import pandas as pd
import numpy as np

# Sample data with NaN and inf
data = {'col1': [1, 2, np.nan, np.inf], 'col2': ['a', 'b', 'c', 'd']}
X_train = pd.DataFrame(data)

# Handle infinite values first
for col in X_train.select_dtypes(include=np.number).columns:
    if np.isinf(X_train[col]).any():
        X_train[col][np.isinf(X_train[col])] = np.nan

# Check for NaN values
for col in X_train.columns:
    if X_train[col].isnull().any():
        if pd.api.types.is_numeric_dtype(X_train[col]):
            # Calculate mean only if column is numeric and has non-NaN values
            if X_train[col].dropna().size > 0:
                mean_value = X_train[col].mean()
                X_train[col].fillna(mean_value, inplace=True)
            else:
                # Handle empty columns (fill with 0 in this example)
                X_train[col].fillna(0, inplace=True)
        else:
            # Handle non-numeric columns (e.g., fill with mode)
            mode_value = X_train[col].mode()[0]
            X_train[col].fillna(mode_value, inplace=True)

print(X_train)

# Check for object columns
!pip install scikit-learn
from sklearn.preprocessing import OneHotEncoder
object_cols = X_train.select_dtypes(include='object').columns
if len(object_cols) > 0:
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_cols = encoder.fit_transform(X_train[object_cols])
    encoded_df = pd.DataFrame(encoded_cols)
    X_train = X_train.drop(object_cols, axis=1)
    X_train = pd.concat([X_train, encoded_df], axis=1)

# Sample data with NaN and inf
data = {'col1': [1, 2, np.nan, np.inf, 5, 6, 7, 8],
        'col2': ['a', 'b', 'c', 'd', 'a', 'b', 'c', 'd'],
        'target': [0, 1, 0, 1, 0, 1, 0, 1]}
df = pd.DataFrame(data)

# Separate features (X) and target (y)
X = df.drop('target', axis=1)
y = df['target']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

# Ensure X_test has the same columns as X_train after encoding
for col in X_train.columns:
    if col not in X_test.columns:
        X_test[col] = 0
        X_test[col] = X_test[col].astype(X_train[col].dtype)

if X_train.shape[0] != y_train.shape[0]:
    # Handle inconsistent shapes (e.g., raise an error or investigate)
    raise ValueError("Inconsistent shapes between X_train and y_train")

object_cols = X_train.select_dtypes(include=['object', 'category']).columns

# Check for object columns
object_cols = X_train.select_dtypes(include=['object', 'category']).columns
if len(object_cols) > 0:
    from sklearn.preprocessing import OneHotEncoder
    encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
    encoded_cols = encoder.fit_transform(X_train[object_cols])
    encoded_df = pd.DataFrame(encoded_cols)
    X_train = X_train.drop(object_cols, axis=1)
    X_train = pd.concat([X_train, encoded_df], axis=1)

# Check for object columns
object_cols = X_train.select_dtypes(include=['object', 'category']).columns
if len(object_cols) > 0:
    from sklearn.preprocessing import LabelEncoder
    encoder = LabelEncoder()
    for col in object_cols:
        X_train[col] = encoder.fit_transform(X_train[col])

# Ensure all column names are strings
X_train.columns = X_train.columns.astype(str)

# Handle column mismatch between X_train and X_test
for col in X_train.columns:
    if col not in X_test.columns:
        X_test[col] = 0

# Ensure X_test has the same order of columns as X_train
X_test = X_test[X_train.columns]

from sklearn.preprocessing import StandardScaler

# Initialize the scaler
scaler = StandardScaler()

# Replace infinite values with NaN
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)

# Fill NaN values with the mean of the column
X_train['col1'].fillna(X_train['col1'].mean(), inplace=True)

# Ensure the same preprocessing for X_test
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test['col1'].fillna(X_test['col1'].mean(), inplace=True)

# Now fit the scaler on X_train
scaler = StandardScaler()
scaler.fit(X_train[['col1']])

# Transform both X_train and X_test
X_train[['col1']] = scaler.transform(X_train[['col1']])
X_test[['col1']] = scaler.transform(X_test[['col1']])

# Replace infinite values with NaN
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)

# Impute NaN values with the mean of the column
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')

X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)

# Standardize the data
scaler = StandardScaler()
X_train[['col1']] = scaler.fit_transform(X_train[['col1']])
X_test[['col1']] = scaler.transform(X_test[['col1']])

# Align y_train with X_train by removing extra rows in X_train
X_train = X_train.iloc[:6, :]  # Limit X_train to 6 rows (to match y_train length)

print(X_train.shape)  # Now should be (6, 4)
print(y_train.shape)  # Should still be (6,)

# Check for rows with NaN in both X_train and y_train, and remove them
common_indices = X_train.dropna().index.intersection(y_train.dropna().index)

# Filter both X_train and y_train using the common indices
X_train = X_train.loc[common_indices]
y_train = y_train.loc[common_indices]

print(X_train.shape)
print(y_train.shape)

# Drop rows with NaN in both X_train and y_train
X_train = X_train.dropna()
y_train = y_train.loc[X_train.index]  # Ensure both have the same index after dropping

print(X_train.shape)
print(y_train.shape)

# Continue with model training and evaluation
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Print classification report
print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# After training the model and making predictions

# Make predictions
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")

# Print confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(conf_matrix)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

